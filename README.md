### **基于Graph Transformer与自我对弈的异构环境任务调度框架**

#### 1. 引言与问题定义

**1.1. 引言**
本方案旨在解决一类在并行与分布式计算领域中基础且核心的**异构计算环境下的有向无环图（DAG）任务调度问题**。其核心目标是找出一个最优的“任务-处理器”映射及时序安排，以**最小化应用的整体完成时间（Makespan）**。该问题作为一类经典的NP-hard问题，其精确解的计算复杂度随问题规模呈指数增长，使得穷举搜索在实际中不可行。

传统的调度方法主要依赖于启发式算法（如HEFT），这些算法通过人工设计的规则在多项式时间内构造出高质量的次优解。然而，启发式算法的局限性在于其规则的普适性有限，难以适应多样化和复杂的任务图结构，其性能距离理论最优值仍有显著差距。

本方案提出一个基于深度强化学习的自适应调度框架，其最终目标并非仅仅超越现有的启发式算法，而是**通过一种从零开始的自我学习范式，探索调度问题的内在结构，以期发现能够逼近理论最优解的调度策略**。我们旨在构建一个能够从问题的基本物理定义中自主学习和推理的智能体，从而在广阔的解空间中定位更高质量的解。

**1.2. 问题规范化定义**
一个调度问题实例由一个任务图`G`和一个异构处理器系统`P`构成，其参数范围经过精心设计以覆盖多样化且实际的场景。

*   **任务图 `G = (T, E)`**:
    *   **任务集合 `T`**: 包含`M`个计算任务, **`M ∈ [5, 15]`**。每个任务`t_i`具有**工作量 `w_i`**，`w_i ∈ [1, 100]`。
    *   **依赖关系 `E`**: 边的集合。每条边`e_{ij}`具有**通信成本 `c_{ij}`**。通信成本的生成与任务工作量相耦合，由**通信计算比 (Communication-to-Computation-Ratio, CCR)**控制，确保了通信与计算之间关系的现实性。本方案考虑`CCR ∈ [0.1, 10.0]`的广泛场景。

*   **处理器系统 `P`**:
    *   **处理器集合 `P`**: 包含`K`个**异构**处理器, **`K ∈ [2, 6]`**。每个处理器`p_k`具有一个**处理速度 `s_k`**，`s_k ∈ [0.5, 2.0]`。因此，任务`t_i`在处理器`p_k`上的执行时间为 `exec(t_i, p_k) = w_i / s_k`。

#### 2. 整体架构与核心思想

本框架采用端到端的强化学习范式。其核心是一个**Graph Transformer**网络，作为策略与价值的统一函数逼近器。该网络指导一个**带狄利克雷噪声的蒙特卡洛树搜索（MCTS）**进行在线规划。智能体通过**自我对弈**生成多样化的调度经验，并利用**优先经验回放**、**自动课程学习**及**并行化**等技术进行高效、稳定地训练。

#### 3. 输入表示：异构图编码（任务与处理器作为不同节点类型）

**3.1. 特征工程的哲学**
在特征工程的设计上，我们面临一个类似AlphaGo Lee（融合人类先验知识）与AlphaGo Zero（从零开始）的根本性抉择。

*   **纯粹主义范式 (Primitivism Paradigm)**：仅使用问题的最原始定义，优点是理论上不为模型的性能设定上限，使其有可能发现超越所有已知启发式的、全新的调度策略。其代价是学习的起点极低，需要巨大的样本复杂度和计算资源才能收敛，学习效率较低。
*   **混合主义范式 (Hybrid Paradigm)**：在原始特征的基础上，融入经过领域专家数十年研究验证的高效启发式特征。其优点在于为模型注入了极具价值的先验知识，显著加速了学习进程，使其能更快地达到一个高性能基准。其潜在风险是可能引入人类的认知偏见，从而在理论上限制模型探索某些反直觉但可能全局更优的策略空间。

**本方案的抉择：**
对于调度这一具有深刻物理意义和优化理论基础的领域，**混合主义范式是实现高性能与高效率平衡的更优选择**。与围棋的抽象博弈不同，调度问题的核心要素与最优解结构之间存在强关联，而“向上秩”等启发式特征正是对这种关联的高度抽象与概括。我们的目标是训练一个能超越现有最佳启发式的智能体，而非要求其从零“重新发明”调度理论。因此，本方案将**融合原始物理特征与高级启发式特征**，并信任Graph Transformer的强大表示能力，来学习如何动态地、情境化地权衡与利用这些信息。

**3.2. 最终特征集与归一化**

*   **任务节点`t_i`的特征向量 `X_t`**:
    1.  **[原始] `w_i`**: 任务的基础工作量。
    2.  **[状态] `status_one_hot`**: One-hot编码，表示任务是`[未完成, 就绪, 已完成]` (3维)。
    3.  **[启发式] `rank_u_i`**: **向上秩 (Upward Rank)**。在异构环境下，其计算遵循HEFT标准定义：`rank_u_i = mean_exec_time_i + max_{t_j ∈ succ(t_i)} (c_{ij} + rank_u_j)`，其中`mean_exec_time_i`是任务`i`在所有处理器上的平均执行时间。**此特征为静态先验知识**，在每个自我对弈Episode开始前计算一次。
    4.  **[动态] `earliest_finish_time_i`**: **针对当前“就绪”任务**，模拟将其调度到所有处理器上所能得到的理论最早完成时间。此特征为模型区分不同就绪任务的紧迫性提供了关键的动态信息。对于非就绪任务，该值为0。

*   **处理器节点`p_k`的特征向量 `X_p`**:
    1.  **[动态] `time_available_k`**: 处理器`k`下一次变为空闲的绝对时间点。
    2.  **[原始] `speed_k`**: 处理器`k`的计算速度。这是区分异构处理器的核心静态特征。

*   **任务-任务边 `e_{ij}` 的特征向量 `E_tt`**:
    1.  **[原始] `c_{ij}`**: 任务`i`到`j`的通信成本。

**归一化策略**:
采用一种**基于问题实例的、分类型的动态归一化**策略：
1.  **预计算统计量**: 在每个自我对弈的episode开始前，计算该实例中所有相关数值的统计量（均值`μ`和标准差`σ`）。
2.  **应用Z-Score归一化**: 对所有**原始特征**和**启发式特征**应用Z-score归一化：`x' = (x - μ) / (σ + ε)`。
3.  **时间相关特征的归一化**: 所有与**时间**相关的**动态特征**将除以当前问题的**HEFT makespan (`M_HEFT`)**。

#### 4. 核心组件设计

*   **神经网络 `f_θ` (Graph Transformer)**:
    *   **编码器**: 多层异构图Transformer。它负责学习图中每个任务和处理器的丰富嵌入表示（`task_embeds`, `proc_embeds`），这些嵌入向量编码了图的结构和当前状态。
    *   **归一化**: Layer Normalization。
    *   **输出头 (分解式策略与价值)**: 网络的输出被分解为三个部分，以高效处理复杂的组合动作空间：
        1.  **任务选择头 (Task Head)**: 输入所有任务节点的最终嵌入`task_embeds`，输出一个覆盖所有`M`个任务的logits分布 `L_t`。经过Softmax后，这代表了选择每个任务的先验概率 `P(t)`。
        2.  **处理器选择头 (Processor Head)**: 这是一个**条件概率**头。它接收**一个任务的嵌入**和**一个处理器的嵌入**拼接而成的向量 `[task_embed_i, proc_embed_k]` 作为输入，输出一个单一的logit，代表将任务`i`分配给处理器`k`的相对优劣。
        3.  **价值头 (Value Head)**: 对所有任务嵌入和处理器嵌入分别进行全局平均池化，然后将两者拼接，最后通过一个MLP输出一个标量`v`，预测当前状态的期望回报（归一化makespan）。

*   **动作构建与掩码 (Action Masking)**:
    在任何决策点，动作构建流程如下：
    1.  系统首先确定**就绪任务集 `T_ready`**（即所有前驱任务均已完成的未调度任务）。
    2.  一个掩码向量被应用于任务选择头的logits `L_t`，将所有非就绪任务的logit设置为一个巨大的负数，确保智能体**永远只会从合法的、可执行的任务中进行选择**。得到合法的任务概率`P(t | t ∈ T_ready)`。
    3.  对于每一个就绪任务 `t_i ∈ T_ready`，模型会动态地计算其与所有处理器 `p_k` 的配对logits `L(p_k | t_i)`。这是通过将其`task_embed_i`与每个`proc_embed_k`组合并传入处理器选择头完成的。
    4.  最终，一个完整的动作（一个`{任务, 处理器}`对）的联合概率 `π(t_i, p_k)` 由 `P(t_i | t_i ∈ T_ready) * P(p_k | t_i)` 计算得出。

*   **MCTS模块**:
    *   **探索**: 根节点策略加入狄利克雷噪声，以鼓励在自我对弈初期进行更广泛的探索。
    *   **评估**: 在MCTS的推演过程中，当到达一个叶节点时，会调用Graph Transformer网络进行一次前向传播，获取该节点的策略和价值，用于指导后续的搜索和价值反向传播。

#### 5. 算法主流程

**初始化**:
1.  随机初始化Graph Transformer网络`f_θ`。
2.  初始化**优先经验回放（PER）**缓冲区`B`。
3.  初始化**自动课程学习**模块，设定初始课程`C_0`。
4.  准备一个固定的、多样化的、能均匀覆盖整个问题空间的**验证集**。对于此验证集中的实例，**使用Gurobi等整数线性规划（ILP）求解器预先计算出其理论最优makespan (`M_ILP`)**，并同时计算HEFT解（`M_HEFT`）作为基准。

**主循环 (并行化)**:
*   **中央学习器 (Learner)**: 运行在主GPU上，负责训练。
*   **多个并行执行器 (Actors)**: 运行在多个CPU核心上，负责生成数据。

**并行执行器 (Actor) 流程 (循环执行)**:
1.  **同步模型**: 从学习器获取最新的网络权重`f_θ`。
2.  **自我对弈生成数据**:
    a. 根据当前课程`C_i`生成问题实例，计算`M_HEFT`。
    b. 对每个实例，执行自我对弈。在每个决策点`t`，通过MCTS生成改善后的策略`π_t`。
    c. 整局游戏结束后，得到最终的归一化makespan `z`。将游戏历史中的每一步都与最终结果配对，生成经验元组`(s_t, π_t, z)`。
    d. 将经验数据异步地发送给中央学习器的PER缓冲区`B`。

**中央学习器 (Learner) 流程 (循环执行)**:
1.  **训练网络**: 持续地从`B`中优先采样数据批次，在主GPU上进行网络训练，最小化策略和价值的损失。
2.  **训练监控与验证**:
    a. **实时训练指标**: 在训练的每个步骤，使用TensorBoard等工具记录以下核心指标：
        *   **`Loss/Total`, `Loss/Policy`, `Loss/Value`**: 总损失及各分量。
    b. **定期验证**: 每隔`N`个训练步，在**固定的验证集**上进行评估，记录：
        *   **`Validation/Avg_SLR_vs_HEFT`**: 平均调度长度比 `mean(M_agent / M_HEFT)`，衡量相对于主流启发式算法的性能。
        *   **`Validation/Win_Rate_vs_HEFT`**: `M_agent < M_HEFT`的问题百分比。
        *   **`Validation/Avg_Optimality_Gap`**: 在包含最优解的验证子集上，计算平均最优间隙 `mean(M_agent / M_ILP)`，**直接量化模型与理论最优解的差距**。
3.  **自动课程学习**:
    a. **课程阶段定义**: 预先定义一系列**循序渐进、范围扩大的**离散课程阶段 `C_0, C_1, ..., C_{final}`。每个阶段`C_i`在涵盖前一阶段所有问题复杂度的基础上，扩展到更难的分布。例如：
        *   `C_0`: `M ∈ [5,8]`, `K ∈ [2,3]`, `CCR ∈ [0.5, 2.0]`
        *   `C_1`: `M ∈ [5,12]`, `K ∈ [2,4]`, `CCR ∈ [0.2, 5.0]`
        *   ...
        *   `C_{final}`: `M ∈ [5,15]`, `K ∈ [2,6]`, `CCR ∈ [0.1, 10.0]` (覆盖整个问题空间)
    b. **科学的触发条件**: **当且仅当**模型在当前课程`C_i`对应的验证子集上的**`Validation/Avg_SLR_vs_HEFT`**，在连续`E`个评估周期（例如`E=5`）内**稳定地保持在1.0以下**时，触发课程晋级。
    c. **晋级机制**: 当触发条件满足时，将所有Actor生成问题的分布，从当前课程`C_i`切换到下一课程`C_{i+1}`。

#### 6. 推理阶段

当模型在最终课程`C_{final}`对应的验证集上的性能收敛后，对于一个新的调度问题：
1.  在每个决策点，构建并归一化状态图`s_t`。
2.  运行MCTS（无噪声，贪婪决策），输出最终调度方案。