# main.py

import torch
import torch.multiprocessing as mp
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch_geometric.data import Batch
import torch.nn.functional as F
import time
import os
import json
import pickle
from collections import deque, namedtuple
import numpy as np
import queue
from tqdm import tqdm

import config
from model import GraphTransformer
from mcts import MCTS
from replay_buffer import PrioritizedReplayBuffer
from dag_generator import generate_dag
from scheduling_env import SchedulingEnv
from validator import Validator
from heuristics import heft_scheduler, calculate_rank_u
from normalization import Normalizer


# --- ä¸“ä¸šæ—¥å¿—è®°å½•å™¨ ---
class Logger:
    """ä¸€ä¸ªç”¨äºç”Ÿæˆä¸“ä¸šã€ç¾è§‚ã€å±‚æ¬¡åŒ–æ—¥å¿—çš„è¾…åŠ©ç±»ã€‚"""

    def __init__(self):
        self._major_step = 0
        self._sub_step = 0

    def _reset_sub_step(self):
        self._sub_step = 0

    def log_major_step(self, message, icon="âš™ï¸"):
        self._major_step += 1
        self._reset_sub_step()
        print(f"\n[{self._major_step}.] {icon}  {message}")
        print("-" * (len(message) + 6))

    def log_sub_step(self, message):
        self._sub_step += 1
        print(f"  {self._major_step}.{self._sub_step}  {message}")

    def log_info(self, message, indent=1, symbol="â””â”€"):
        prefix = "   " * indent
        print(f"{prefix}{symbol} {message}")

    def log_event(self, message, icon="ğŸ””", border_char="="):
        print(f"\n{border_char * 3} {icon}  {message}  {icon} {border_char * 3}")

    def print_header(self, title):
        print("\n" + "=" * 60)
        print(f"{' ' * ((60 - len(title)) // 2)}{title}")
        print("=" * 60)

    def print_footer(self):
        print("=" * 60)


# å®šä¹‰ç»éªŒå…ƒç»„
Experience = namedtuple('Experience', ('state', 'pi', 'value'))
# å®šä¹‰æ¨ç†è¯·æ±‚å…ƒç»„ï¼Œéœ€åŒ…å«action_mask
InferenceRequest = namedtuple('InferenceRequest', ('request_id', 'state_data', 'action_mask'))


def actor_process(actor_id, data_queue, curriculum_queue, stop_event,
                  inference_request_queue, inference_result_dict):
    """
    å¹¶è¡Œæ‰§è¡Œå™¨ (Actor) çš„å·¥ä½œæµç¨‹ã€‚
    ä½œä¸ºæ¨ç†å®¢æˆ·ç«¯ï¼Œä¸æŒæœ‰æ¨¡å‹ï¼Œé€šè¿‡é˜Ÿåˆ—ä¸æ¨ç†å·¥ä½œå™¨é€šä¿¡ã€‚
    """
    pid = os.getpid()
    print(f"[ACTOR {actor_id} | PID: {pid}] â–¶ï¸  Process started.")

    mcts = MCTS(request_queue=inference_request_queue,
                result_dict=inference_result_dict)
    current_curriculum = 0

    while not stop_event.is_set():
        # éé˜»å¡åœ°æ£€æŸ¥è¯¾ç¨‹æ›´æ–°
        while not curriculum_queue.empty():
            try:
                message = curriculum_queue.get_nowait()
                if 'curriculum' in message:
                    current_curriculum = message['curriculum']
                    print(f"[ACTOR {actor_id}] ğŸ”„ Updated to Curriculum: {current_curriculum}")
            except queue.Empty:
                break

        # --- ç”Ÿæˆä¸€ä¸ªè°ƒåº¦é—®é¢˜å®ä¾‹ ---
        m_range, k_range, ccr_range = config.CURRICULUM_STAGES[current_curriculum]
        dag, k, proc_speeds = generate_dag(m_range, k_range, ccr_range)
        calculate_rank_u(dag, proc_speeds)
        heft_makespan = heft_scheduler(dag.copy(), k, proc_speeds)
        normalizer = Normalizer(dag, heft_makespan, proc_speeds)

        env = SchedulingEnv(dag, k, proc_speeds, normalizer)
        state = env.reset()
        game_history = []

        # --- è‡ªæˆ‘å¯¹å¼ˆ ---
        while True:
            action_mask = env.get_action_mask()
            if not np.any(action_mask):
                break  # æ²¡æœ‰åˆæ³•åŠ¨ä½œï¼Œæ¸¸æˆç»“æŸ

            # ä½¿ç”¨MCTSå¢å¼ºç­–ç•¥
            pi, _ = mcts.search(env, config.TRAINING_MCTS_SIMULATIONS, env.heft_makespan)

            if not pi:
                break  # MCTSæœªè¿”å›æœ‰æ•ˆç­–ç•¥

            # ä¿å­˜çŠ¶æ€å’ŒMCTSè¾“å‡ºçš„ç­–ç•¥
            full_pi = np.zeros((env.num_tasks, env.k))
            for (task, proc), prob in pi.items():
                full_pi[task, proc] = prob
            game_history.append({'state': state.clone(), 'pi': full_pi})

            # æ ¹æ®MCTSç­–ç•¥é‡‡æ ·åŠ¨ä½œ
            actions, probs = list(pi.keys()), list(pi.values())
            # ä¸ºnp.random.choiceç¡®ä¿æ¦‚ç‡å’Œä¸º1 (å¤„ç†æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜)
            probs = np.array(probs, dtype=np.float64)
            probs /= np.sum(probs)
            action_idx = np.random.choice(len(actions), p=probs)
            action = actions[action_idx]

            state, _, done = env.step(action)
            if done:
                break

        # --- æ¸¸æˆç»“æŸåï¼Œæ•´ç†å¹¶å‘é€ç»éªŒ ---
        if game_history:
            final_makespan = env.get_makespan()
            # ä»·å€¼è¢«å½’ä¸€åŒ–ï¼Œä½œä¸ºæ‰€æœ‰æ­¥éª¤çš„å…±åŒç›®æ ‡
            normalized_value = -final_makespan / heft_makespan if heft_makespan > 0 else -1.0
            experiences_to_send = [Experience(transition['state'], transition['pi'], normalized_value)
                                   for transition in game_history]
            data_queue.put(experiences_to_send)


def inference_worker(model_dict, request_queue, result_dict, stop_event, model_params):
    """
    ä¸€ä¸ªä¸“é—¨åœ¨GPUä¸Šè¿è¡Œæ¨¡å‹æ¨ç†çš„ç‹¬ç«‹è¿›ç¨‹ï¼Œä½œä¸ºæ¨ç†æœåŠ¡å™¨ã€‚
    æ­¤ç‰ˆæœ¬ç»è¿‡ä¼˜åŒ–ï¼Œä½¿ç”¨æ¨¡å‹å†…ç½®æ–¹æ³•æ¥è®¡ç®—ç­–ç•¥ï¼Œä»¥æé«˜ä»£ç å¤ç”¨æ€§ã€‚
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    pid = os.getpid()
    print(f"[INFERENCE WORKER | PID: {pid}] â–¶ï¸  Process started on {device}.")

    model = GraphTransformer(**model_params).to(device)
    model.eval()

    # åˆå§‹åŠ è½½ä¸€æ¬¡æ¨¡å‹
    try:
        cpu_state_dict = model_dict.copy()
        if cpu_state_dict:
            model.load_state_dict(cpu_state_dict)
    except Exception as e:
        print(f"[INFERENCE WORKER] Initial model load failed: {e}")

    while not stop_event.is_set():
        # --- å®‰å…¨åœ°è·å–æœ€æ–°çš„æ¨¡å‹çŠ¶æ€ ---
        # model_dict.copy() æ˜¯åŸå­æ“ä½œ, ä¿è¯äº†è·å–åˆ°çš„æ˜¯ä¸€ä¸ªå®Œæ•´çš„çŠ¶æ€å¿«ç…§
        cpu_state_dict = model_dict.copy()
        if not cpu_state_dict:  # å¦‚æœlearnerè¿˜æ²¡æ¥å¾—åŠæ”¾å…¥æ¨¡å‹ï¼Œåˆ™ç¨ç­‰
            time.sleep(0.1)
            continue
        model.load_state_dict(cpu_state_dict)

        # --- æ‰¹é‡æ”¶é›†æ¨ç†è¯·æ±‚ ---
        requests = []
        try:
            while len(requests) < config.BATCH_SIZE:
                req = request_queue.get_nowait()
                requests.append(req)
        except queue.Empty:
            if not requests:
                time.sleep(0.001)  # é˜Ÿåˆ—ä¸ºç©ºæ—¶çŸ­æš‚ä¼‘çœ ï¼Œé¿å…CPUç©ºè½¬
                continue

        # --- æ‰§è¡Œæ‰¹é‡æ¨ç† ---
        if requests:
            batch = Batch.from_data_list([req.state_data for req in requests]).to(device)
            with torch.no_grad():
                task_logits_all, value_preds, task_embeds_all, proc_embeds_all = model(batch)

            task_ptr = batch['task'].ptr
            proc_ptr = batch['proc'].ptr

            # --- è§£ææ‰¹æ¬¡ç»“æœå¹¶è¿”å›ç»™å„ä¸ªActor ---
            for i, req in enumerate(requests):
                start_task, end_task = task_ptr[i], task_ptr[i + 1]
                start_proc, end_proc = proc_ptr[i], proc_ptr[i + 1]

                single_task_logits = task_logits_all[start_task:end_task]
                single_value = value_preds[i].item()
                single_task_embeds = task_embeds_all[start_task:end_task]
                single_proc_embeds = proc_embeds_all[start_proc:end_proc]
                action_mask = torch.from_numpy(req.action_mask).to(device)

                # è°ƒç”¨æ¨¡å‹å†…ç½®æ–¹æ³•è®¡ç®—ç­–ç•¥å­—å…¸ï¼Œæ¶ˆé™¤äº†æ­¤å¤„çš„é‡å¤ä»£ç 
                policy_dict = model.compute_policy_dict(
                    single_task_logits,
                    single_task_embeds,
                    single_proc_embeds,
                    action_mask
                )

                result_dict[req.request_id] = (policy_dict, single_value)


def save_checkpoint(state):
    """ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œå°†replay bufferåˆ†å¼€å­˜å‚¨ä»¥é¿å…å†…å­˜é—®é¢˜ã€‚"""
    buffer_to_save = state.pop('replay_buffer', None)
    torch.save(state, config.CHECKPOINT_FILE)
    if buffer_to_save:
        with open(config.REPLAY_BUFFER_FILE, "wb") as f:
            pickle.dump(buffer_to_save, f)


def load_checkpoint(model, optimizer):
    """åŠ è½½æ£€æŸ¥ç‚¹ï¼Œå¹¶åˆ†å¼€åŠ è½½replay bufferã€‚"""
    if os.path.isfile(config.CHECKPOINT_FILE):
        try:
            # weights_only=Falseæ˜¯é»˜è®¤å€¼ï¼Œä½†æ˜¾å¼æŒ‡å‡ºæ˜¯ä¸ºäº†æ¸…æ™°åœ°è¡¨æ˜æˆ‘ä»¬è¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ç­‰éæƒé‡ä¿¡æ¯
            checkpoint = torch.load(config.CHECKPOINT_FILE, map_location=config.DEVICE, weights_only=False)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if os.path.isfile(config.REPLAY_BUFFER_FILE):
                with open(config.REPLAY_BUFFER_FILE, "rb") as f:
                    replay_buffer = pickle.load(f)
                return checkpoint, replay_buffer, True
            else:
                print(f"  [WARNING] Checkpoint found but replay buffer file '{config.REPLAY_BUFFER_FILE}' is missing.")
                # å³ä½¿æ²¡æœ‰bufferï¼Œä¹Ÿè¿”å›checkpointä¿¡æ¯ï¼Œä»¥ä¾¿æ¢å¤global_stepç­‰
                return checkpoint, None, True
        except (KeyError, pickle.UnpicklingError, RuntimeError) as e:
            print(
                f"  [WARNING] Failed to load checkpoint file. It might be corrupt or from an incompatible version: {e}")
            return None, None, False
        except Exception as e:
            print(f"  [WARNING] An unexpected error occurred while loading checkpoint: {e}")
            return None, None, False

    return None, None, False


def update_web_ui_data(data):
    """
    å°†è®­ç»ƒçŠ¶æ€å†™å…¥JSONæ–‡ä»¶ï¼Œä¾›Webå‰ç«¯ç›‘æ§ã€‚
    æ­¤ç‰ˆæœ¬èƒ½æ­£ç¡®å¤„ç†åµŒå¥—çš„ç›‘æ§æ•°æ®ç»“æ„ã€‚
    """
    # åˆ›å»ºä¸€ä¸ªå¯åºåˆ—åŒ–çš„å‰¯æœ¬
    serializable_data = {
        "global_step": data.get("global_step", 0),
        "current_curriculum": data.get("current_curriculum", 0),
        "training_metrics": {k: list(v) for k, v in data.get("training_metrics", {}).items()},
        "system_metrics": {k: list(v) for k, v in data.get("system_metrics", {}).items()}
    }

    # å•ç‹¬å¤„ç†åµŒå¥—çš„éªŒè¯æŒ‡æ ‡
    serializable_validation = {}
    validation_data = data.get("validation_metrics", {})
    for stage_key, stage_data in validation_data.items():
        serializable_validation[stage_key] = {k: list(v) for k, v in stage_data.items()}
    serializable_data["validation_metrics"] = serializable_validation

    try:
        with open(config.WEB_STATUS_FILE, 'w') as f:
            json.dump(serializable_data, f, indent=4)
    except IOError as e:
        print(f"  [WARNING] Web UI status file could not be written: {e}")


def learner_process():
    """å­¦ä¹ å™¨ (Learner) çš„ä¸»å·¥ä½œæµç¨‹ã€‚"""
    logger = Logger()
    logger.print_header("Reinforcement Learning Scheduler - Learner")

    # --- 1. åˆå§‹åŒ– ---
    logger.log_major_step("System Initialization")
    logger.log_sub_step(f"Setting device to: {config.DEVICE}")
    logger.log_sub_step("Checking and creating result directories...")
    dirs_to_create = [config.RESULT_DIR, config.LOG_DIR, config.MODEL_SAVE_DIR, config.WEB_MONITOR_DIR]
    for d in dirs_to_create: os.makedirs(d, exist_ok=True)
    logger.log_info(f"All output will be saved in '{config.RESULT_DIR}'")

    # --- 2. åŠ¨æ€è·å–ç¯å¢ƒç»´åº¦ ---
    logger.log_major_step("Getting Environment Dimensions", icon="ğŸ“")
    try:
        dummy_dag, dummy_k, dummy_speeds = generate_dag(m_range=(5, 5), k_range=(2, 2), ccr_range=(1, 1))
        calculate_rank_u(dummy_dag, dummy_speeds)
        dummy_env = SchedulingEnv(dummy_dag, dummy_k, dummy_speeds)
        dummy_state = dummy_env.reset()
        task_feature_dim = dummy_state['task'].x.shape[1]
        proc_feature_dim = dummy_state['proc'].x.shape[1]
        edge_feature_dim = dummy_state['task', 'depends_on', 'task'].edge_attr.shape[1]
        logger.log_sub_step(f"Task feature dimension: {task_feature_dim}")
        logger.log_sub_step(f"Processor feature dimension: {proc_feature_dim}")
        logger.log_sub_step(f"Edge feature dimension: {edge_feature_dim}")
    except Exception as e:
        logger.log_info(f"âŒ CRITICAL: Failed to create dummy environment to get feature dims: {e}")
        return

    # --- 3. æ¨¡å‹ä¸ä¼˜åŒ–å™¨è®¾ç½® ---
    logger.log_major_step("Model & Optimizer Setup", icon="ğŸ§ ")
    logger.log_sub_step("Creating GraphTransformer model...")
    model_params = {
        "task_feature_dim": task_feature_dim,
        "proc_feature_dim": proc_feature_dim,
        "edge_feature_dim": edge_feature_dim,
        "embedding_dim": config.EMBEDDING_DIM,
        "num_heads": config.NUM_ATTENTION_HEADS,
        "num_layers": config.NUM_ENCODER_LAYERS
    }
    model = GraphTransformer(**model_params).to(config.DEVICE)

    try:
        logger.log_sub_step("Initializing model with a dummy forward pass...")
        model.eval()
        with torch.no_grad():
            model(Batch.from_data_list([dummy_state]).to(config.DEVICE))
        model.train()
        logger.log_info("âœ… Model layers initialized successfully.")
    except Exception as e:
        logger.log_info(f"âŒ CRITICAL: Model initialization failed: {e}")
        return

    logger.log_sub_step(f"Creating Adam optimizer with learning rate {config.LEARNING_RATE}...")
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)

    # --- 4. æ¢å¤è®­ç»ƒçŠ¶æ€ ---
    logger.log_major_step("Restoring Training State", icon="ğŸ’¾")
    logger.log_sub_step(f"Attempting to load checkpoint from '{config.CHECKPOINT_FILE}'...")
    checkpoint, replay_buffer, success = load_checkpoint(model, optimizer)
    if success:
        global_step = checkpoint.get('global_step', 0)
        current_curriculum = checkpoint.get('current_curriculum', 0)
        slr_history = checkpoint.get('slr_history', deque(maxlen=config.PROMOTION_STABLE_EPOCHS))
        if replay_buffer is None:
            replay_buffer = PrioritizedReplayBuffer(config.REPLAY_BUFFER_SIZE, alpha=config.PER_ALPHA)
            logger.log_info("âš ï¸ Replay buffer file was missing. Initializing a new one.")
        logger.log_info(f"âœ… Checkpoint loaded. Resuming from step {global_step}.")
    else:
        global_step, current_curriculum = 0, 0
        slr_history = deque(maxlen=config.PROMOTION_STABLE_EPOCHS)
        replay_buffer = PrioritizedReplayBuffer(config.REPLAY_BUFFER_SIZE, alpha=config.PER_ALPHA)
        logger.log_info(f"âš ï¸ No valid checkpoint found. Starting from scratch.")

    # --- 5. åˆå§‹åŒ–å…¶ä»–ç»„ä»¶ ---
    logger.log_major_step("Initializing Components")
    writer = SummaryWriter(config.LOG_DIR)
    logger.log_sub_step("Setting up TensorBoard SummaryWriter...")
    validator = Validator(config.VALIDATION_SET_SIZE, config.CURRICULUM_STAGES)
    logger.log_sub_step("Setting up Validator with persistent dataset...")

    # æ ¹æ®æ£€æŸ¥ç‚¹æ˜¯å¦å­˜åœ¨æ¥åŠ è½½æˆ–åˆ›å»º web_ui_data
    if success and 'web_ui_data' in checkpoint and checkpoint['web_ui_data']:
        logger.log_info("âœ… Restoring historical monitoring data from checkpoint.")
        web_ui_data = checkpoint['web_ui_data']

        # å…³é”®ä¸€æ­¥: torch.load ä¼šå°† deque å˜å› listã€‚
        # æˆ‘ä»¬éœ€è¦é€’å½’åœ°å°†æ‰€æœ‰æ•°æ®åˆ—è¡¨è½¬æ¢å› dequeï¼Œä»¥ä¾¿åç»­çš„ .append() æ“ä½œèƒ½æ­£å¸¸å·¥ä½œã€‚
        for metric_type in ['training_metrics', 'system_metrics']:
            if metric_type in web_ui_data:
                for key, value in web_ui_data[metric_type].items():
                    web_ui_data[metric_type][key] = deque(value)

        if 'validation_metrics' in web_ui_data:
            for stage_key, stage_data in web_ui_data['validation_metrics'].items():
                for key, value in stage_data.items():
                    web_ui_data['validation_metrics'][stage_key][key] = deque(value)

    else:
        if success:
            logger.log_info(
                "âš ï¸ Checkpoint found, but no historical monitoring data inside. Initializing a new monitoring session.")
        else:
            logger.log_info("âš ï¸ No checkpoint found. Initializing a new monitoring session.")

        # å¦‚æœæ²¡æœ‰æ£€æŸ¥ç‚¹æˆ–æ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰ç›‘æ§æ•°æ®ï¼Œåˆ™ä½¿ç”¨åŸå§‹çš„åˆå§‹åŒ–ä»£ç 
        web_ui_data = {
            "global_step": global_step,
            "current_curriculum": current_curriculum,
            "training_metrics": {
                "steps": deque(), "total_loss": deque(),
                "policy_loss": deque(), "value_loss": deque()
            },
            "system_metrics": {
                "steps": deque(),
                "buffer_size": deque()
            },
            "validation_metrics": {}
        }

    # --- 6. å¯åŠ¨å¹¶è¡Œè¿›ç¨‹ ---
    logger.log_major_step(f"Spawning {config.NUM_ACTORS} Actors & 1 Inference Worker", icon="ğŸš€")
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        logger.log_info("Start method has already been set. Continuing...")
    manager = mp.Manager()

    cpu_model_state = {k: v.cpu().detach() for k, v in model.state_dict().items()}
    model_dict = manager.dict(cpu_model_state)
    data_queue = manager.Queue()
    curriculum_queues = [manager.Queue() for _ in range(config.NUM_ACTORS)]
    stop_event = manager.Event()
    inference_request_queue = manager.Queue()
    inference_result_dict = manager.dict()

    inference_proc = mp.Process(target=inference_worker,
                                args=(
                                    model_dict, inference_request_queue, inference_result_dict, stop_event,
                                    model_params))
    inference_proc.start()

    actors = []
    for i in range(config.NUM_ACTORS):
        curriculum_queues[i].put({'curriculum': current_curriculum})
        actor = mp.Process(target=actor_process,
                           args=(i, data_queue, curriculum_queues[i], stop_event,
                                 inference_request_queue, inference_result_dict))
        actor.start()
        actors.append(actor)
    logger.log_info("âœ… All processes launched.")

    # --- 7. ä¸»è®­ç»ƒå¾ªç¯ ---
    logger.log_major_step("Starting Main Training Loop", icon="â–¶ï¸")

    steps_until_val = config.VALIDATION_INTERVAL
    initial_step = global_step % steps_until_val
    next_val_step = global_step - initial_step + steps_until_val
    pbar = tqdm(total=steps_until_val, initial=initial_step,
                desc=f"Training (next val @ step {next_val_step})",
                bar_format="{l_bar}{bar:20}{r_bar}{bar:-20b}")

    try:
        while True:
            # --- æ•°æ®æ”¶é›† ---
            try:
                while not data_queue.empty():
                    experience_list = data_queue.get_nowait()
                    for exp in experience_list:
                        replay_buffer.add(exp)
            except queue.Empty:
                pass

            # --- ç­‰å¾…ç¼“å†²åŒºå¡«å…… ---
            if len(replay_buffer) < config.MIN_BUFFER_SIZE_FOR_TRAINING:
                print(f"\r[INFO] Filling replay buffer... {len(replay_buffer)}/{config.MIN_BUFFER_SIZE_FOR_TRAINING}",
                      end="")
                time.sleep(0.1)
                continue

            if global_step == 0 and len(replay_buffer) >= config.MIN_BUFFER_SIZE_FOR_TRAINING:
                pbar.close()  # å…³é—­åˆå§‹çš„bar
                print()  # æ¢è¡Œ
                logger.log_info(f"âœ… Replay Buffer filled. Starting training on {config.DEVICE}.")
                # é‡æ–°åˆ›å»ºbarå¼€å§‹æ­£å¼è®¡æ•°
                pbar = tqdm(total=steps_until_val, initial=0,
                            desc=f"Training (next val @ step {steps_until_val})",
                            bar_format="{l_bar}{bar:20}{r_bar}{bar:-20b}")

            # --- é‡‡æ ·ä¸æ‰¹æ¬¡å‡†å¤‡ ---
            model.train()
            beta = min(1.0,
                       config.PER_BETA_START + global_step * (1.0 - config.PER_BETA_START) / config.PER_BETA_FRAMES)
            experiences, indices, weights = replay_buffer.sample(config.BATCH_SIZE, beta)
            batch = Batch.from_data_list([exp.state for exp in experiences]).to(config.DEVICE)

            pis = [exp.pi for exp in experiences]
            max_m = max(pi.shape[0] for pi in pis)
            max_k = max(pi.shape[1] for pi in pis) if any(pi.ndim > 1 and pi.shape[1] > 0 for pi in pis) else 1
            padded_pis = np.zeros((len(pis), max_m, max_k), dtype=np.float32)
            for i, pi in enumerate(pis):
                if pi.ndim > 1 and pi.shape[1] > 0:
                    padded_pis[i, :pi.shape[0], :pi.shape[1]] = pi

            pi_targets_padded = torch.from_numpy(padded_pis).float().to(config.DEVICE)
            value_targets = torch.tensor([exp.value for exp in experiences], dtype=torch.float).view(-1, 1).to(
                config.DEVICE)
            weights_tensor = torch.tensor(weights, dtype=torch.float).view(-1, 1).to(config.DEVICE)

            # --- å‰å‘ä¼ æ’­ä¸æŸå¤±è®¡ç®— ---
            task_logits_all, value_preds, task_embeds_all, proc_embeds_all = model(batch)

            value_loss = (weights_tensor * F.mse_loss(value_preds, value_targets, reduction='none')).mean()

            policy_losses = []
            task_ptr, proc_ptr = batch['task'].ptr, batch['proc'].ptr
            for i in range(batch.num_graphs):
                num_tasks, num_procs = task_ptr[i + 1] - task_ptr[i], proc_ptr[i + 1] - proc_ptr[i]
                task_logits = task_logits_all[task_ptr[i]:task_ptr[i + 1]]
                task_embeds = task_embeds_all[task_ptr[i]:task_ptr[i + 1]]
                proc_embeds = proc_embeds_all[proc_ptr[i]:proc_ptr[i + 1]]
                pi_target = pi_targets_padded[i, :num_tasks, :num_procs]
                pi_task_target = pi_target.sum(dim=1)

                loss_task_i = -torch.sum(pi_task_target * F.log_softmax(task_logits, dim=0))
                loss_proc_i = torch.tensor(0.0, device=config.DEVICE)

                valid_tasks_mask = pi_task_target > 1e-8
                if valid_tasks_mask.any():
                    valid_task_embeds = task_embeds[valid_tasks_mask]
                    num_valid_tasks = valid_task_embeds.shape[0]
                    task_embeds_rep = valid_task_embeds.unsqueeze(1).expand(-1, num_procs, -1)
                    proc_embeds_rep = proc_embeds.unsqueeze(0).expand(num_valid_tasks, -1, -1)

                    combined_embeds = torch.cat([task_embeds_rep, proc_embeds_rep], dim=2).view(-1,
                                                                                                config.EMBEDDING_DIM * 2)
                    proc_logits = model.policy_head_proc(combined_embeds).view(num_valid_tasks, num_procs)

                    log_pi_proc_pred = F.log_softmax(proc_logits, dim=1)
                    pi_proc_target_cond = pi_target[valid_tasks_mask] / (
                            pi_task_target[valid_tasks_mask].unsqueeze(1) + 1e-8)
                    ce_per_task = -torch.sum(pi_proc_target_cond * log_pi_proc_pred, dim=1)
                    loss_proc_i = torch.sum(pi_task_target[valid_tasks_mask] * ce_per_task)

                policy_losses.append(loss_task_i + loss_proc_i)

            policy_loss = (weights_tensor.squeeze() * torch.stack(policy_losses)).mean()
            total_loss = value_loss + policy_loss

            # --- åå‘ä¼ æ’­ä¸ä¼˜åŒ– ---
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

            # --- æ›´æ–°PERä¼˜å…ˆçº§ ---
            td_errors = torch.abs(value_preds - value_targets).detach().cpu().numpy().flatten()
            replay_buffer.update_priorities(indices, td_errors)

            # --- æ—¥å¿—è®°å½• ---
            writer.add_scalar("Loss/Total", total_loss.item(), global_step)
            writer.add_scalar("Loss/Policy", policy_loss.item(), global_step)
            writer.add_scalar("Loss/Value", value_loss.item(), global_step)
            pbar.set_postfix_str(f"Loss={total_loss.item():.3f}")

            web_ui_data["global_step"] = global_step
            web_ui_data["training_metrics"]["steps"].append(global_step)
            web_ui_data["training_metrics"]["total_loss"].append(total_loss.item())
            web_ui_data["training_metrics"]["policy_loss"].append(policy_loss.item())
            web_ui_data["training_metrics"]["value_loss"].append(value_loss.item())
            web_ui_data["system_metrics"]["steps"].append(global_step)
            web_ui_data["system_metrics"]["buffer_size"].append(len(replay_buffer))

            # --- å®šæœŸéªŒè¯ ---
            if global_step > 0 and global_step % config.VALIDATION_INTERVAL == 0:
                pbar.close()
                logger.log_event(f"Step {global_step}: Running Validation on All Learned Curricula", icon="ğŸ“Š")
                val_results_by_stage = validator.evaluate(model, config.DEVICE, current_curriculum, global_step)

                for stage, stage_results in val_results_by_stage.items():
                    # --- æ›´æ–°TensorBoardæ—¥å¿— ---
                    writer.add_scalars(f"Validation_Makespan/C{stage}", {
                        'Agent': stage_results['Avg_Makespan_Agent'],
                        'HEFT': stage_results['Avg_Makespan_HEFT'],
                        'ILP': stage_results['Avg_Makespan_ILP'],
                    }, global_step)
                    writer.add_scalar(f"Validation_SLR/C{stage}", stage_results['Avg_SLR_vs_HEFT'], global_step)
                    writer.add_scalar(f"Validation_Gap/C{stage}", stage_results['Avg_Optimality_Gap'], global_step)
                    writer.add_scalar(f"Validation_WinRate/C{stage}", stage_results['Win_Rate_vs_HEFT'], global_step)

                    # --- æ›´æ–°Web UIæ•°æ® ---
                    stage_key = f"C{stage}"
                    if stage_key not in web_ui_data["validation_metrics"]:
                        web_ui_data["validation_metrics"][stage_key] = {
                            "steps": deque(), "avg_makespan_agent": deque(),
                            "avg_makespan_heft": deque(), "avg_makespan_ilp": deque(),
                            "avg_slr": deque(), "avg_gap": deque(), "win_rate": deque()
                        }

                    ui_stage_data = web_ui_data["validation_metrics"][stage_key]
                    ui_stage_data["steps"].append(global_step)
                    ui_stage_data["avg_makespan_agent"].append(stage_results['Avg_Makespan_Agent'])
                    ui_stage_data["avg_makespan_heft"].append(stage_results['Avg_Makespan_HEFT'])
                    ui_stage_data["avg_makespan_ilp"].append(stage_results['Avg_Makespan_ILP'])
                    ui_stage_data["avg_slr"].append(stage_results['Avg_SLR_vs_HEFT'])
                    ui_stage_data["avg_gap"].append(stage_results['Avg_Optimality_Gap'])
                    ui_stage_data["win_rate"].append(stage_results['Win_Rate_vs_HEFT'])

                # æ‰“å°å½“å‰è¯¾ç¨‹çš„æ‘˜è¦
                current_results = val_results_by_stage[current_curriculum]
                logger.log_info(f"Summary for Current Curriculum (C{current_curriculum}):", symbol="â””â”€")
                agent_m = current_results['Avg_Makespan_Agent']
                heft_m = current_results['Avg_Makespan_HEFT']
                ilp_m = current_results['Avg_Makespan_ILP']
                makespan_summary_line = f"Avg. Makespans (Agent | HEFT   | ILP)   : {agent_m:>7.2f} | {heft_m:>7.2f} | {ilp_m:>7.2f}"
                logger.log_info(makespan_summary_line, indent=2)
                logger.log_info(f"Avg. SLR vs HEFT: {current_results['Avg_SLR_vs_HEFT']:.4f}", indent=2)
                logger.log_info(f"Avg. Optimality Gap: {current_results['Avg_Optimality_Gap']:.4f}", indent=2)

                # --- è¯¾ç¨‹å­¦ä¹ æ™‹çº§åˆ¤æ–­ (ä»…åŸºäºå½“å‰è¯¾ç¨‹çš„è¡¨ç°) ---
                slr_history.append(current_results['Avg_SLR_vs_HEFT'])
                if len(slr_history) == config.PROMOTION_STABLE_EPOCHS and all(
                        s < config.PROMOTION_THRESHOLD_SLR for s in slr_history):
                    if current_curriculum < max(config.CURRICULUM_STAGES.keys()):
                        current_curriculum += 1
                        web_ui_data["current_curriculum"] = current_curriculum
                        logger.log_event(f"CURRICULUM PROMOTION! -> Stage {current_curriculum}", icon="â­",
                                         border_char="â­")
                        for q in curriculum_queues: q.put({'curriculum': current_curriculum})
                        slr_history.clear()

                pbar = tqdm(total=steps_until_val, initial=0,
                            desc=f"Training (next val @ step {global_step + steps_until_val})",
                            bar_format="{l_bar}{bar:20}{r_bar}{bar:-20b}")

            # --- æ¨¡å‹åŒæ­¥ ---
            if global_step > 0 and global_step % config.MODEL_SYNC_INTERVAL == 0:
                current_cpu_state = {k: v.cpu().detach() for k, v in model.state_dict().items()}
                model_dict.update(current_cpu_state)

            # --- ä¿å­˜æ£€æŸ¥ç‚¹ ---
            if global_step > 0 and global_step % config.CHECKPOINT_INTERVAL == 0:
                logger.log_event(f"Step {global_step}: Saving Checkpoint", icon="ğŸ’¾")
                save_checkpoint({
                    'global_step': global_step,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'current_curriculum': current_curriculum,
                    'slr_history': slr_history,
                    'replay_buffer': replay_buffer,
                    'web_ui_data': web_ui_data
                })
                logger.log_info(f"âœ… Checkpoint saved to '{config.CHECKPOINT_FILE}'.")

            update_web_ui_data(web_ui_data)
            global_step += 1
            if len(replay_buffer) >= config.MIN_BUFFER_SIZE_FOR_TRAINING:
                pbar.update(1)

    except KeyboardInterrupt:
        logger.log_event("KeyboardInterrupt caught. Initiating graceful shutdown...", icon="ğŸ›‘")
    finally:
        if 'pbar' in locals() and pbar:
            pbar.close()
        logger.log_major_step("Shutdown Sequence", icon="ğŸ›‘")
        logger.log_sub_step("Sending stop signal to all processes...")
        stop_event.set()

        logger.log_info("Waiting for Inference Worker to terminate...", indent=2)
        inference_proc.join(timeout=5)
        logger.log_info("âœ… Inference Worker process has been terminated.")
        logger.log_info("Waiting for Actor processes to terminate...", indent=2)
        for actor in actors: actor.join(timeout=5)
        logger.log_info("âœ… All Actor processes have been terminated.")

        logger.log_sub_step("Saving final model state and replay buffer...")
        save_checkpoint({
            'global_step': global_step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'current_curriculum': current_curriculum,
            'slr_history': slr_history,
            'replay_buffer': replay_buffer,
            'web_ui_data': web_ui_data
        })
        logger.log_info(f"âœ… Final state saved to '{config.CHECKPOINT_FILE}'.")
        logger.print_footer()


if __name__ == '__main__':
    learner_process()