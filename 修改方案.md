### **AERL-DS (Asynchronous Evolution-Reinforcement Learning for DAG Scheduling) - 最终修订版方案**

#### **1. 系统概述与设计哲学**

**1.1. 纲要**
AERL-DS (Asynchronous Evolution-Reinforcement Learning for DAG Scheduling) 是一个专为解决异构环境下有向无环图（DAG）任务调度问题而设计的混合式学习框架。它系统地结合了强化学习（RL）的策略学习能力、进化策略（ES）的并行探索与选择机制，以及异步并行计算的高效性，旨在训练出一个能够生成接近理论最优调度方案的神经网络模型。

**1.2. 核心设计原则**
*   **统一协作探索（Unified Collaborative Exploration）**: 针对一个调度问题，所有并行的`Actor`不再进行独立的、重复的自我对弈，而是协作构建**一个单一的、共享的蒙特卡洛搜索树**。此设计从根本上消除了冗余探索，并将所有计算资源都集中在对问题解空间的同一次、深度集中的分析上。
*   **预算控制的探索深度（Budget-Controlled Exploration Depth）**: 系统的核心探索参数是一个统一的**“总模拟预算”**，它直接定义了对一个问题投入的总计算量，以确保探索的充分性。
*   **保证轨迹完整性（Guaranteed Trajectory Completion）**: 系统的探索机制必须**保证**在仲裁和评估前，已经成功发现了足够数量的、从起始到终止的完整调度轨迹。
*   **后验轨迹生成与评估（Posterior Trajectory Generation & Evaluation）**: 在统一的搜索树构建完毕后，系统会从这棵蕴含了大量探索信息的“专家知识树”中**后验地采样**出多个高质量、高多样性的调度轨迹，并对这些轨迹进行确定性评估以获得真实的`makespan`。
*   **内部相对适应度（Internal Relative Fitness）**: 学习信号源于这些后验生成的轨迹之间的内部、相对`makespan`比较，完全摆脱对外部启发式基准的依赖。
*   **基数信息保留（Cardinal Information Preservation）**: 回报函数不仅反映解决方案的**序数排名**，还必须反映它们之间性能差距的**基数信息**，以提供更丰富、更精确的信用分配。

---

#### **2. 系统架构与组件职责**

系统由四个核心组件构成，运行在一个多进程环境中。

| 组件 | 运行环境 | 实例数量 | 核心职责 |
| :--- | :--- | :--- | :--- |
| **`Learner`** | 主进程 (GPU) | 1 | 神经网络训练、任务生命周期管理、后验轨迹生成与评估、结果仲裁与回报计算。 |
| **`Actor`** | 子进程 (CPU) | `NUM_ACTORS` | **协作树构建者**。执行单一的MCTS模拟步，并将其更新到共享的搜索树中，同时负责报告完整轨迹的发现。 |
| **`Inference Worker`**| 子进程 (GPU) | 1 | 为所有`Actor`的MCTS模拟提供批处理的、低延迟的神经网络推理服务。 |
| **`Validator`** | `Learner`进程内模块| 1 | 周期性地、确定性地评估模型在固定验证集上的性能。 |

---

#### **3. 详细工作流程与机制**

##### **3.1. `Learner`进程：中央协调与学习核心**

*   **3.1.1. 超参数定义**:
    *   `NUM_ACTORS`: 并行`Actor`的总数。
    *   **`MCTS_SIMULATION_BUDGET`**: 固定的MCTS模拟总预算（例如，10000）。
    *   **`MIN_TRAJECTORIES_TO_FIND`**: 必须发现的最小完整轨迹数（例如，64）。
    *   `TEMPERATURE (τ)`: 用于softmax加权排名回报的温度参数（例如，0.1）。
    *   `BATCH_SIZE`, `LEARNING_RATE`等标准RL超参数。

*   **3.1.2. 共享数据结构初始化**:
    *   `task_registry = manager.dict()`: 全局任务注册中心，结构为`{task_id: {'params': ..., 'lock': ..., 'sims_done': 0, 'terminal_paths_found': 0, 'shared_tree': manager.dict(), 'budget': MCTS_SIMULATION_BUDGET, 'min_paths': MIN_TRAJECTORIES_TO_FIND}}`。
    *   `task_id_queue = manager.Queue()`: 存放待分配给`Actor`的`task_id`。
    *   其他组件：`Replay Buffer`, `model`, `optimizer`。

*   **3.1.3. 任务生命周期管理 (在独立的管理线程中运行)**:
    *   **a. 任务创建**: 持续创建新任务，并在`task_registry`中注册，最后将`task_id`放入`task_id_queue`。
    *   **b. 后验轨迹生成、评估与数据打包 (核心仲裁逻辑)**:
        *   **触发条件 (AND逻辑)**: 定期扫描`task_registry`，当发现某个`task_id`**同时满足**以下两个条件时，触发仲裁：
            1.  `sims_done >= MCTS_SIMULATION_BUDGET`
            2.  `terminal_paths_found >= MIN_TRAJECTORIES_TO_FIND`
        *   **锁定与获取**: 锁定并获取该`task_id`的完整`shared_tree`，然后从注册中心删除该任务。
        *   **轨迹采样**:
            1.  在一个循环中，执行`MIN_TRAJECTORIES_TO_FIND`次。
            2.  **单次采样**: 从`shared_tree`的根节点开始，根据节点中存储的访问次数（`N(s,a)`）进行**概率加权采样**来选择一个动作，进入下一层节点。重复此过程，直到生成一条完整的动作序列（一条轨迹）。
            3.  将采样的`(state, MCTS_policy)`序列存储为`game_history`。
        *   **轨迹评估**:
            1.  对于刚才采样出的每一条轨迹（动作序列）。
            2.  在一个**本地的、确定性的模拟环境**中，按顺序执行该轨迹中的所有动作。
            3.  记录下最终的真实`makespan`。
        *   **回报计算**: 应用**4.1节**中的**值加权的softmax排名回报**公式，为每一条轨迹计算其最终的回报`reward`。
        *   **打包与存储**: 将所有轨迹的经验数据，根据其各自的回报，打包成`Experience`元组，并全部添加到`Replay Buffer`中。

*   **3.1.4. 模型训练**: 从`Replay Buffer`中进行优先经验采样，计算损失并更新模型参数。

##### **3.2. `Actor`进程：协作式树搜索工作者**

`Actor`的职责被高度简化和统一，专注于为共享树贡献计算量。

*   **3.2.1. 主工作循环**:
    *   **a. 领取任务**: 从`task_id_queue`中获取一个`task_id`。
    *   **b. 协作构建循环**:
        *   在一个`while`循环中，持续为该`task_id`贡献MCTS模拟。
        *   **循环终止条件**: `Actor`在每次迭代前，**检查该`task_id`是否仍然存在于`task_registry`中**。如果`Learner`已经仲裁并删除了该任务，`Actor`就应立即终止对此任务的工作，并返回领取新任务。
        *   **单次MCTS模拟步**:
            1.  **安全地访问共享树**: 获取该`task_id`的锁。
            2.  **执行一次完整的Select-Expand-Backpropagate**:
                *   **Select**: 从`shared_tree`的根节点开始，根据PUCT规则，递归选择子节点，直到到达一个叶子节点或**终止状态节点（所有任务已调度）**。
                *   **Expand & Evaluate**:
                    *   **IF** 到达的是非终止的叶子节点: 通过`Inference Worker`获取其策略和价值`v`，并用这些信息扩展`shared_tree`。
                    *   **IF** 到达的是终止状态节点: **价值`v`被确定性地设为`0`**，因为它没有未来奖励。
                *   **Backpropagate**: 将得到的价值`v`（预测值或0）沿路径向上反向传播。此步骤的核心作用有两个：
                    1.  **更新动作价值Q(s,a)**: 将价值`v`累加到路径上所有节点的`Q`值统计中。
                    2.  **更新访问计数N(s,a)**: **无论`v`的值是多少（包括0）**，路径上所有节点的访问次数`N`都必须**加一**。这是MCTS记录探索信息的基础，**绝对不能省略**。
                *   **IF** 到达的是终止状态节点: 在反向传播后，额外将共享的`terminal_paths_found`计数加一。
            3.  **更新计数并释放**: 将`sims_done`计数加一，然后释放锁。

---

#### **4. 核心算法与参数设计**

##### **4.1. 回报函数设计：值加权的Softmax排名回报**

*   **目标**: 结合排名的序数信息和`makespan`的基数信息，生成科学的、尺度不变的回报。
*   **输入**: 一批`K`个从完成后验采样的轨迹中评估出的`makespan`值 `M = [m_1, m_2, ..., m_K]`。
*   **步骤**:
    1.  **Z-Score归一化**:
        *   **公式**: `m'_i = (m_i - μ_M) / (σ_M + ε)`
        *   **原理**: 消除`makespan`的绝对尺度，使其围绕0分布，具有统一的方差，便于后续处理。
    2.  **Softmax加权**:
        *   **公式**: `w_i = softmax(-m' / τ)_i`
        *   **原理**: 将归一化的`makespan`（负值，因为越小越好）转换为一个概率分布。`τ` (温度)控制了分布的尖锐度：`τ`越小，权重越集中在最好的解上，更能体现精英主义。
    3.  **最终回报归一化**:
        *   **公式**: `reward_i = (w_i - μ_w) / (σ_w + ε)`
        *   **原理**: 对权重本身再次进行Z-Score归一化，产生一个均值为0、方差为1的稳定回报信号，非常有利于神经网络的训练。

---

#### **5. 课程学习与验证机制**

##### **5.1. `Validator`模块**
*   **职责**:
    *   在固定的`global_step`间隔时同步调用。
    *   模型切换到`eval()`模式，在**固定的验证集**上运行。
    *   决策过程使用**确定性的MCTS**（无噪声，贪心选择）。
*   **输出**:
    *   为每个课程等级计算`Avg_SLR_vs_HEFT`等性能指标，用于人工监控和课程晋级决策。

##### **5.2. 课程晋级机制**
*   **数据**: `Learner`为每个课程等级`C_i`维护独立的性能历史队列`performance_history[i]`。
*   **晋级条件 (必须同时满足)**:
    1.  **性能基准达标**: `current_slr` 必须小于 `VALIDATION_SLR_THRESHOLD`。
    2.  **稳定进步确认**: 对`performance_history[i]`中的数据进行统计分析（例如，线性回归斜率为负，且近期方差小于阈值），确认性能处于稳定或提升状态。
*   **执行**: 若满足条件，则`current_curriculum += 1`，`Learner`开始分发更高难度的任务。

---

#### **6. 特征工程**
*   **输入特征**:
    *   **物理特征**: 任务工作量、通信成本、处理器速度。
    *   **动态状态特征**: 任务状态（uncompleted/ready/completed）、处理器可用时间、任务理论最早完成时间（EFT）。
    *   **启发式特征**: 向上秩`rank_u`。